{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Video Object Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-12T19:37:28.888530Z",
     "start_time": "2017-10-12T19:37:27.537749Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "\"\"\" import dependencies / necessary packages \"\"\"\n",
    "import scipy\n",
    "import time \n",
    "import math \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wave, struct\n",
    "import matplotlib.pyplot as plt\n",
    "# import the necessary packages\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import argparse\n",
    "import cv2\n",
    "\n",
    "from IPython.display import display\n",
    "from scipy.io import wavfile as wav\n",
    "from scipy import signal\n",
    "\n",
    "# Show matplotlib plots inline (nicely formatted in the notebook)\n",
    "%matplotlib inline\n",
    "\n",
    "print ('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To-Do\n",
    "    1. Get image snips\n",
    "        - Ears, Nose, Tail\n",
    "    2. Download and train classifier\n",
    "    3. Write code to do live classifying \n",
    "    ____________________________________\n",
    "    \n",
    "    4. Edit code to get coordinates \n",
    "    5. Use coordinates to get directions, relative positions\n",
    "        - facing, proximity relative to body size, alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Video images from video to label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-12T19:39:30.980858Z",
     "start_time": "2017-10-12T19:39:30.937744Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Frame:\n",
      "40223\n",
      "Image Width:\n",
      "640\n",
      "Image Height:\n",
      "480\n",
      "Frames-Per-Second:\n",
      "30.0\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/33650974/opencv-python-read-specific-frame-using-videocapture\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "#Get video name from user\n",
    "#Ginen video name must be in quotes, e.g. \"pirkagia.avi\" or \"plaque.avi\"\n",
    "#Please give the video name including its extension. E.g. \\\"pirkagia.avi\\\":\\n\n",
    "# video_name = input(\"Please give the video name including its extension. E.g. \\\"pirkagia.avi\\\":\\n\")\n",
    "\n",
    "video_name = (r\"C:/Users/leviz/OneDrive/School/PhD/projects/Video/260104_100607_24hREUNION.avi\")\n",
    "\n",
    "\"\"\" set desired image type (jpg, png)\"\"\"\n",
    "image_type = \"jpg\"\n",
    "#Open the video file\n",
    "cap = cv2.VideoCapture(video_name)\n",
    "\n",
    "\n",
    "if not cap.isOpened(): \n",
    "    print (\"could not open video_name\")\n",
    "    \n",
    "\n",
    "#Entire video length information\n",
    "num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps    = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "print (\"Number of Frame:\")\n",
    "print (num_frames)\n",
    "print (\"Image Width:\")\n",
    "print (width)\n",
    "print (\"Image Height:\")\n",
    "print (height)\n",
    "print (\"Frames-Per-Second:\")\n",
    "print (fps)\n",
    "\n",
    "#Set frame_no in range 0.0-1.0\n",
    "#For example, if we have a video of 30 seconds having 25 frames per seconds, we thus would have 750 frames.\n",
    "#The examined frame must get a value from 0 to 749.\n",
    "#For more info about the video flags see here: https://stackoverflow.com/questions/11420748/setting-camera-parameters-in-opencv-python\n",
    "#Here we calculate the last frame as frame sequence=749. \n",
    "\n",
    "#In case you want to select other frame change value 749.\n",
    "#BE CAREFUL! Each video has different time length and frame rate. \n",
    "#So make sure that you have the right parameters for the right video!\n",
    "\n",
    "#Comment out and change to desired length if the orignial is not what you want to sample\n",
    "#Example: change fps to 10 if original is 20fps and you want to sample at 10fps\n",
    "# time_length = 30.0\n",
    "# fps=.00001\n",
    "# frame_seq = time_length*fps-1\n",
    "# frame_no = (frame_seq /(time_length*fps))\n",
    "\n",
    "time_length = num_frames / fps\n",
    "frame_seq = num_frames-1\n",
    "frame_no = (frame_seq /(time_length*fps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T22:12:50.672260Z",
     "start_time": "2017-10-11T22:12:22.602244Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count=1\n",
    "while(cap.isOpened()):\n",
    "    frameId= cap.get(1)\n",
    "    #Read the next frame from the video. If you set frame 749 above then the code will return the last frame.\n",
    "    ret, frame = cap.read()\n",
    "    if ret == True:\n",
    "        if (count % math.floor(fps*2) != 0):\n",
    "            count+=1\n",
    "            \n",
    "        if (count % math.floor(fps*2) == 0):\n",
    "            # define range of blue, red, green color in HSV\n",
    "    #         lower_red = np.array([20,0,0])\n",
    "    #         upper_red = np.array([255,255,255])\n",
    "\n",
    "            lower_green = np.array([0,0,0])\n",
    "            upper_green = np.array([255,255,255])\n",
    "\n",
    "            lower_blue = np.array([0,0,0])\n",
    "            upper_blue = np.array([255,255,170])\n",
    "\n",
    "            #Set grayscale or HSV colorspace for the frame. \n",
    "    #         gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    #         color = cv2.cvtColor(frame, cv2.COLOR_GRAY2RGB)\n",
    "            hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "            hsv4 = cv2.cvtColor(frame, cv2.COLOR_RGB2HLS)\n",
    "    #         hsv7 = cv2.cvtColor(frame, cv2.COLOR_RGB2XYZ)\n",
    "\n",
    "            # add color masks\n",
    "    #         hsv_red = cv2.inRange(hsv, lower_red, upper_red)\n",
    "    #         hsv_green = cv2.inRange(hsv, lower_green, upper_green)\n",
    "            hsv_blue = cv2.inRange(hsv, lower_blue, upper_blue)\n",
    "\n",
    "    #         res_red = cv2.bitwise_and(hsv4,hsv4, mask= hsv_red)\n",
    "    #         res_green = cv2.bitwise_and(hsv4,hsv4, mask= hsv_green)\n",
    "    #         res_blue = cv2.bitwise_and(hsv4,hsv4, mask= hsv_blue)\n",
    "\n",
    "    #         res_red = cv2.bitwise_and(frame,frame, mask= hsv_red)\n",
    "    #         res_green = cv2.bitwise_and(frame,frame, mask= hsv_green)\n",
    "            res_blue = cv2.bitwise_and(frame,frame, mask= hsv_blue)        \n",
    "\n",
    "            #Cut the video extension to have the name of the video\n",
    "            my_video_name = video_name.split(\".\")[0]\n",
    "\n",
    "    #         cv2.imshow(\"original\", frame)\n",
    "    #         cv2.imshow('HSV', hsv)\n",
    "    #         cv2.imshow('HSV4', hsv4)\n",
    "    # #         cv2.imshow('HSV7', hsv7)\n",
    "\n",
    "    #         cv2.imshow(\"res red\", res_red)\n",
    "    #         cv2.imshow(\"res green\", res_green)\n",
    "    #         cv2.imshow(\"res blue\", res_blue)\n",
    "\n",
    "    #         cv2.imshow(\"red\", hsv_red)\n",
    "    #         cv2.imshow('green', hsv_green)\n",
    "    #         cv2.imshow(\"blue\", hsv_blue)\n",
    "    #         cv2.imshow('green', hsv_green)\n",
    "\n",
    "            my_video_name = video_name.split(\".\")[0]\n",
    "\n",
    "\n",
    "            cv2.imwrite(my_video_name+ \"_frame_\"+str(count)+'.'+image_type,res_blue)\n",
    "\n",
    "            count += 1\n",
    "        if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "            cv2.destroyAllWindows()\n",
    "            break\n",
    "        \n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-12T19:52:59.229786Z",
     "start_time": "2017-10-12T19:50:46.528549Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "            \n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture('260104_100607_24hREUNION.avi')\n",
    "fps = int(fps)\n",
    "# Define the codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'DIVX')\n",
    "out = cv2.VideoWriter('output.mp4',fourcc, fps, (width,height))\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    if ret==True:\n",
    "        lower_blue = np.array([0,0,0])\n",
    "        upper_blue = np.array([255,255,170])\n",
    "        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "        hsv_blue = cv2.inRange(hsv, lower_blue, upper_blue)\n",
    "        res_blue = cv2.bitwise_and(frame,frame, mask= hsv_blue)\n",
    "        \n",
    "        frame = res_blue\n",
    "        # write the flipped frame\n",
    "        out.write(frame)\n",
    "\n",
    "#         cv2.imshow('frame',frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Release everything if job is finished\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-12T19:49:31.085337Z",
     "start_time": "2017-10-12T19:49:31.064281Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press [ESC] to quit demo\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T21:50:14.915945Z",
     "start_time": "2017-10-11T21:50:08.891396Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "while(cap.isOpened()):\n",
    "\n",
    "    #Read the next frame from the video. If you set frame 749 above then the code will return the last frame.\n",
    "    ret, frame = cap.read()\n",
    "    if ret == True:\n",
    "        \n",
    "        # define range of blue, red, green color in HSV\n",
    "#         lower_red = np.array([20,0,0])\n",
    "#         upper_red = np.array([255,255,255])\n",
    "        \n",
    "        lab= cv2.cvtColor(frame, cv2.COLOR_BGR2LAB)\n",
    "        cv2.imshow(\"lab\",lab)\n",
    "\n",
    "        #-----Splitting the LAB image to different channels-------------------------\n",
    "        l, a, b = cv2.split(lab)\n",
    "        cv2.imshow('l_channel', l)\n",
    "        cv2.imshow('a_channel', a)\n",
    "        cv2.imshow('b_channel', b)\n",
    "\n",
    "        #-----Applying CLAHE to L-channel-------------------------------------------\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "        cl = clahe.apply(l)\n",
    "        cv2.imshow('CLAHE output', cl)\n",
    "\n",
    "        #-----Merge the CLAHE enhanced L-channel with the a and b channel-----------\n",
    "        limg = cv2.merge((cl,a,b))\n",
    "        cv2.imshow('limg', limg)\n",
    "        \n",
    "        #-----Converting image from LAB Color model to RGB model--------------------\n",
    "        final = cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)\n",
    "        cv2.imshow('final', final)\n",
    "\n",
    "        hsv4 = cv2.cvtColor(final, cv2.COLOR_RGB2HLS)\n",
    "\n",
    "        lower_blue = np.array([0,0,0])\n",
    "        upper_blue = np.array([255,255,120])\n",
    "        hsv_blue = cv2.inRange(final, lower_blue, upper_blue)\n",
    "        res_blue = cv2.bitwise_and(final,final, mask= hsv_blue)        \n",
    "        cv2.imshow(\"res blue\", res_blue)\n",
    "        \n",
    "        if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "            cv2.destroyAllWindows()\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.opencv.org/3.2.0/df/d9d/tutorial_py_colorspaces.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#The first argument of cap.set(), number 2, defines that parameter for setting the frame selection.\n",
    "#Number 2 defines flag CV_CAP_PROP_POS_FRAMES which is a 0-based index of the frame to be decoded/captured next.\n",
    "#The second argument defines the frame number in range 0.0-1.0\n",
    "# cap.set(1,frame_no);\n",
    "count=1\n",
    "while(cap.isOpened()):\n",
    "\n",
    "    #Read the next frame from the video. If you set frame 749 above then the code will return the last frame.\n",
    "    ret, frame = cap.read()\n",
    "    if ret == True:\n",
    "        \n",
    "        #Set grayscale or HSV colorspace for the frame. \n",
    "        \n",
    "#         gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "\n",
    "        \n",
    "        #Cut the video extension to have the name of the video\n",
    "        my_video_name = video_name.split(\".\")[0]\n",
    "\n",
    "        #Uncomment the line below to Display the resulting frame\n",
    "        # cv2.imshow(my_video_name+' frame '+ str(frame_seq),gray)\n",
    "        \n",
    "        #Store this frame to an image\n",
    "#         cv2.imwrite(my_video_name+'_frame_'+str(frame_seq)+'.'+image_type,gray)\n",
    "#         cv2.imwrite(my_video_name+'_frame_'+str(count)+'.'+image_type,gray)\n",
    "        cv2.imwrite(my_video_name+ \"_frame_\"+str(count)+'.'+image_type,hsv)\n",
    "\n",
    "        count += 1\n",
    "#Set waitKey \n",
    "# cv2.waitKey()\n",
    "        if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "            print (\"waitKey reached. Break.\")\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# sign tracking - warped boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\" sign tracking \"\"\"\n",
    "\n",
    "# import the necessary packages\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "#Difference Variable\n",
    "minDiff = 10000\n",
    "minSquareArea = 5000\n",
    "match = -1\n",
    "\n",
    "#Frame width & Height\n",
    "w=640\n",
    "h=480\n",
    "\n",
    "#Reference Images Display name & Original Name\n",
    "#ReferenceImages = [\"ArrowL.jpg\",\"ArrowR.jpg\",\"ArrowT.jpg\",\"Ball.jpg\",\"Go.jpg\",\"Stop.jpg\"]\n",
    "#ReferenceTitles = [\"Turn Left 90\",\"Turn Right 90\",\"Turn Around\",\"Search for Ball\",\"Start..\",\"Stop!\"]\n",
    "\n",
    "#define class for References Images\n",
    "class Symbol:\n",
    "    def __init__(self):\n",
    "        self.img = 0\n",
    "        self.name = 0\n",
    "\n",
    "#define class instances (6 objects for 6 different images)\n",
    "symbol= [Symbol() for i in range(6)]\n",
    "\n",
    "\n",
    "\n",
    "def readRefImages():\n",
    "    for count in range(6):\n",
    "        image = cv2.imread(ReferenceImages[count], cv2.COLOR_BGR2GRAY)\n",
    "        symbol[count].img = cv2.resize(image,(w/2,h/2),interpolation = cv2.INTER_AREA)\n",
    "        symbol[count].name = ReferenceTitles[count]\n",
    "        #cv2.imshow(symbol[count].name,symbol[count].img);\n",
    "\n",
    "\n",
    "def order_points(pts):\n",
    "        # initialzie a list of coordinates that will be ordered\n",
    "        # such that the first entry in the list is the top-left,\n",
    "        # the second entry is the top-right, the third is the\n",
    "        # bottom-right, and the fourth is the bottom-left\n",
    "        rect = np.zeros((4, 2), dtype = \"float32\")\n",
    "\n",
    "        # the top-left point will have the smallest sum, whereas\n",
    "        # the bottom-right point will have the largest sum\n",
    "        s = pts.sum(axis = 1)\n",
    "        rect[0] = pts[np.argmin(s)]\n",
    "        rect[2] = pts[np.argmax(s)]\n",
    "\n",
    "        # now, compute the difference between the points, the\n",
    "        # top-right point will have the smallest difference,\n",
    "        # whereas the bottom-left will have the largest difference\n",
    "        diff = np.diff(pts, axis = 1)\n",
    "        rect[1] = pts[np.argmin(diff)]\n",
    "        rect[3] = pts[np.argmax(diff)]\n",
    "\n",
    "        # return the ordered coordinates\n",
    "        return rect\n",
    "\n",
    "def four_point_transform(image, pts):\n",
    "        # obtain a consistent order of the points and unpack them\n",
    "        # individually\n",
    "        rect = order_points(pts)\n",
    "        (tl, tr, br, bl) = rect\n",
    "\n",
    "        maxWidth = w/2\n",
    "        maxHeight = h/2\n",
    "\n",
    "        dst = np.array([\n",
    "                [0, 0],\n",
    "                [maxWidth - 1, 0],\n",
    "                [maxWidth - 1, maxHeight - 1],\n",
    "                [0, maxHeight - 1]], dtype = \"float32\")\n",
    "\n",
    "        # compute the perspective transform matrix and then apply it\n",
    "        M = cv2.getPerspectiveTransform(rect, dst)\n",
    "        warped = cv2.warpPerspective(image, M, (maxWidth, maxHeight))\n",
    "\n",
    "        # return the warped image\n",
    "        return warped\n",
    "\n",
    "\n",
    "def auto_canny(image, sigma=0.33):\n",
    "        # compute the median of the single channel pixel intensities\n",
    "        v = np.median(image)\n",
    "\n",
    "        # apply automatic Canny edge detection using the computed median\n",
    "        lower = int(max(0, (1.0 - sigma) * v))\n",
    "        upper = int(min(255, (1.0 + sigma) * v))\n",
    "        edged = cv2.Canny(image, lower, upper)\n",
    "\n",
    "        # return the edged image\n",
    "        return edged\n",
    "\n",
    "\n",
    "def resize_and_threshold_warped(image):\n",
    "        #Resize the corrected image to proper size & convert it to grayscale\n",
    "        #warped_new =  cv2.resize(image,(w/2, h/2))\n",
    "        warped_new_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        #Smoothing Out Image\n",
    "        blur = cv2.GaussianBlur(warped_new_gray,(5,5),0)\n",
    "\n",
    "        #Calculate the maximum pixel and minimum pixel value & compute threshold\n",
    "        min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(blur)\n",
    "        threshold = (min_val + max_val)/2\n",
    "\n",
    "        #Threshold the image\n",
    "        ret, warped_processed = cv2.threshold(warped_new_gray, threshold, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "        #return the thresholded image\n",
    "        return warped_processed\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    #Font Type\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "\n",
    "    # initialize the camera and grab a reference to the raw camera capture\n",
    "    video = cv2.VideoCapture(0)\n",
    "\n",
    "    #Windows to display frames\n",
    "    cv2.namedWindow(\"Main Frame\", cv2.WINDOW_AUTOSIZE)\n",
    "    cv2.namedWindow(\"Matching Operation\", cv2.WINDOW_AUTOSIZE)\n",
    "    cv2.namedWindow(\"Corrected Perspective\", cv2.WINDOW_AUTOSIZE)\n",
    "    cv2.namedWindow(\"Contours\", cv2.WINDOW_AUTOSIZE)\n",
    "\n",
    "    #Read all the reference images\n",
    "    readRefImages()\n",
    "\n",
    "    # capture frames from the camera\n",
    "    while True:\n",
    "            ret, OriginalFrame = video.read()\n",
    "            gray = cv2.cvtColor(OriginalFrame, cv2.COLOR_BGR2GRAY)\n",
    "            blurred = cv2.GaussianBlur(gray,(3,3),0)\n",
    "\n",
    "            #Detecting Edges\n",
    "            edges = auto_canny(blurred)\n",
    "\n",
    "            #Contour Detection & checking for squares based on the square area\n",
    "            cntr_frame, contours, hierarchy = cv2.findContours(edges,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "            for cnt in contours:\n",
    "                    approx = cv2.approxPolyDP(cnt,0.01*cv2.arcLength(cnt,True),True)\n",
    "\n",
    "                    if len(approx)==4:\n",
    "                            area = cv2.contourArea(approx)\n",
    "\n",
    "                            if area > minSquareArea:\n",
    "                                    cv2.drawContours(OriginalFrame,[approx],0,(0,0,255),2)\n",
    "                                    warped = four_point_transform(OriginalFrame, approx.reshape(4, 2))\n",
    "                                    warped_eq = resize_and_threshold_warped(warped)\n",
    "\n",
    "\n",
    "                                    for i in range(6):\n",
    "                                        diffImg = cv2.bitwise_xor(warped_eq, symbol[i].img)\n",
    "                                        diff = cv2.countNonZero(diffImg);\n",
    "\n",
    "                                        if diff < minDiff:\n",
    "                                            match = i\n",
    "\n",
    "                                            #print symbol[i].name, diff\n",
    "                                            #print approx.reshape(4,2)[0]\n",
    "                                            #cv2.putText(OriginalFrame,symbol[i].name, (10,30), font, 1, (255,0,255), 2, cv2.LINE_AA)\n",
    "                                            cv2.putText(OriginalFrame,symbol[i].name, tuple(approx.reshape(4,2)[0]), font, 1, (255,0,255), 2, cv2.LINE_AA)\n",
    "                                            diff = minDiff\n",
    "                                            break;\n",
    "\n",
    "                                    cv2.imshow(\"Corrected Perspective\", warped_eq)\n",
    "                                    cv2.imshow(\"Matching Operation\", diffImg)\n",
    "                                    cv2.imshow(\"Contours\", edges)\n",
    "\n",
    "            #Display Main Frame\n",
    "            cv2.imshow(\"Main Frame\", OriginalFrame)\n",
    "\n",
    "            k = cv2.waitKey(1) & 0xFF\n",
    "            if k == 27:\n",
    "                    break\n",
    "\n",
    "    video.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "#Run Main\n",
    "if __name__ == \"__main__\" :\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
